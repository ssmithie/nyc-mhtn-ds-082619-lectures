{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "import matplotlib\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "matplotlib.rcParams['text.color'] = 'G'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis: SARIMA Model\n",
    "\n",
    "## Agenda:\n",
    "\n",
    "- ACF and PACF\n",
    "- Fitting the Model\n",
    "- Evaluating the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'monthly_sales' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-98ccedaa0350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseasonal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseasonal_decompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdecomposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseasonal_decompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonthly_sales\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'monthly_sales' is not defined"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(monthly_sales, freq=12)\n",
    "fig = plt.figure()\n",
    "fig = decomposition.plot()\n",
    "fig.set_size_inches(15, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#create a function that will help us to quickly \n",
    "def test_stationarity(timeseries, window):\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window=window).mean()\n",
    "    rolstd = timeseries.rolling(window=window).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    orig = plt.plot(timeseries.iloc[window:], color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the stationarity of the untransformed dataset\n",
    "test_stationarity(monthly_sales.payment_amount, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice:\n",
    "\n",
    "Take the following time series data and try to make it as stationary as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transit = pd.read_csv('./resources/portland-oregon-average-monthly-.csv', index_col=0)\n",
    "df_transit.columns = ['riders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transit.index[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_transit.drop(df_transit.index[114], inplace=True)\n",
    "df_transit['riders']= df_transit['riders'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_transit.plot(figsize=(9,6), title= 'Monthly Ridership', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Students will be able to build and evaluate an ARIMA based model using ACF and PACF plots to choose the appropriate terms of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agenda:**  \n",
    "- Explain AR and MA terms\n",
    "- Explain ACF and PACF charts\n",
    "- Build an SARIMA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great Resources:\n",
    "\n",
    "- https://newonlinecourses.science.psu.edu/stat510/lesson/1\n",
    "- https://people.duke.edu/~rnau/411arim.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up data for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./resources/Sales_Data.csv', sep='\\t', index_col=0)\n",
    "df['purchase_date']= pd.to_datetime(df['purchase_date']) \n",
    "df = df[df['payment_amount']>100]\n",
    "daily_sales = df.groupby(['purchase_date'])['payment_amount'].sum()\n",
    "monthly_sales = daily_sales.resample('m').sum()\n",
    "monthly_sales = pd.DataFrame(monthly_sales, columns=['payment_amount'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Models\n",
    "There are two basic types of “time domain” models:\n",
    "\n",
    "- Models that relate the present value of a series to past values and past prediction errors - these are called **ARIMA** models (for Autoregressive Integrated Moving Average). We’ll spend substantial time on these.\n",
    "- Ordinary regression models that use time indices as x-variables. These can be helpful for an initial description of the data and form the basis of several simple forecasting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a linear regression model with stationary time series data\n",
    "\n",
    "You can take your newly stationary data and use it as your target variable for a linear regression model. \n",
    "\n",
    "Making your data stationary tries to remove the correlation between serial time points. There still can be soem element of time that can explain changes in your target variable. A leap year, for example, will add an extra day for sales in February.  \n",
    "\n",
    "1. How should we account for this in our linear regression model?\n",
    "2. What are other time components that we might want to account for in a linear regression model? (think about other examples of hourly, daily, or weekly data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***When performing a linear regression model on time series data, you need to eventually transformyour data back to the original scale***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversing a first difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with random numbers\n",
    "df_diff = pd.DataFrame({'A': np.random.randint(0, 10, 10)})\n",
    "#create the first difference column\n",
    "df_diff['B'] = df_diff['A'].diff()\n",
    "\n",
    "#Reverse the first difference\n",
    "x, x_diff = df_diff['A'].iloc[0], df_diff['B'].iloc[1:]\n",
    "df_diff['C'] = np.r_[x, x_diff].cumsum().astype(int)\n",
    "\n",
    "#view the data\n",
    "df_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to creating an Arima model for time series data\n",
    "![steps](./resources/flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Integrated Moving Average  (ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoregressive term\n",
    "\n",
    "If the series is stationary and autocorrelated, perhaps it can be predicted as a multiple of its own previous value, plus a constant.  The forecasting equation in this case is:\n",
    "\n",
    "\n",
    "$$Ŷ_t  =  μ  +  ϕ_{t-1}Y_{t-1}$$\n",
    "\n",
    "…which is Y regressed on itself lagged by one period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Moving Average term\n",
    "\n",
    "Another strategy for correcting autocorrelated errors is to use an average of the last few observations in order to filter out the noise and more accurately estimate the local mean. One of which is the so-called “error correction” form, in which the previous forecast is adjusted in the direction of the error it made:\n",
    "\n",
    "$$Ŷ_t   =  Ŷ_{t-1}  + αe_{t-1}$$  \n",
    "\n",
    "Because $e_{t-1} = Y_{t-1} - Ŷ_{t-1}$ by definition, this can be rewritten as:\n",
    "\n",
    "$$Ŷ_t   =  Y_{t-1}  - (1-α)e_{t-1}$$\n",
    "\n",
    "$$Ŷ_t=  Y_{t-1}  - θ_1e_{t-1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated\n",
    "\n",
    "Refers to the order of differencing used in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An ARIMA model is classified as an **ARIMA(p,d,q)** model, \n",
    "\n",
    "    **p** = number of autoregressive (AR) terms \n",
    "    **d** = number of differences \n",
    "    **q** = number of moving average (MA) terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal ARIMA model with Exogenous Factors  (SARIMAX)\n",
    "\n",
    "A seasonal ARIMA model is classified as an **ARIMA(p,d,q)x(P,D,Q)** model, \n",
    "\n",
    "    **p** = number of autoregressive (AR) terms \n",
    "    **d** = number of differences \n",
    "    **q** = number of moving average (MA) terms\n",
    "     \n",
    "    **P** = number of seasonal autoregressive (SAR) terms \n",
    "    **D** = number of seasonal differences \n",
    "    **Q** = number of seasonal moving average (SMA) terms\n",
    "    \n",
    "**Exogenous Factors** refers to any features that are not derived from the target variable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One we have achieved stationarity the next step in fitting a model to address any autocorrelation that remains in the differenced series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way is just to run all combinations of options and see which one performs the best by some metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = q = range(0, 2)\n",
    "pdq = list(itertools.product(p, [1], q))\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, [1], q))]\n",
    "print('Examples of parameter for SARIMA...')\n",
    "for i in pdq:\n",
    "    for s in seasonal_pdq:\n",
    "        print('SARIMAX: {} x {}'.format(i, s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = monthly_sales['payment_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(y,order=param,seasonal_order=param_seasonal,enforce_stationarity=False,enforce_invertibility=False)\n",
    "            results = mod.fit()\n",
    "            print('ARIMA{}x{}12 - AIC:{}'.format(param,param_seasonal,results.aic))\n",
    "        except: \n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ACF/PACF charts and find optimal parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation Function ACF\n",
    "\n",
    "\n",
    "The sample autocorrelation function (ACF) for a series gives correlations between the series  and lagged values of the series for lags of 1, 2, 3, and so on. The lagged values can be written as , and so on. The ACF gives correlations between  and ,  and , and so on.\n",
    "\n",
    "The ACF can be used to identify the possible structure of time series data. That can be tricky going as there often isn’t a single clear-cut interpretation of a sample autocorrelation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial autocorrelation \n",
    "\n",
    "In general, a partial correlation is a conditional correlation. It is the correlation between two variables under the assumption that we know and take into account the values of some other set of variables. For instance, consider a regression context in which y is the response variable and , , and  are predictor variables. The partial correlation between y and  is the correlation between the variables determined taking into account how both y and  are related to  and .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACF and PACF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "#plot the ACF\n",
    "fig = sm.graphics.tsa.plot_acf(monthly_sales.payment_amount.iloc[1:], lags=18, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "#plot the PACF\n",
    "fig = sm.graphics.tsa.plot_pacf(monthly_sales.payment_amount.iloc[1:], lags=18, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales['first_difference'] = monthly_sales.payment_amount - monthly_sales.payment_amount.shift(1)  \n",
    "monthly_sales['seasonal_first_difference'] = monthly_sales.first_difference - monthly_sales.first_difference.shift(12)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on first difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "#plot the ACF\n",
    "fig = sm.graphics.tsa.plot_acf(monthly_sales.first_difference.iloc[1:], lags=18, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "#plot the PACF\n",
    "fig = sm.graphics.tsa.plot_pacf(monthly_sales.first_difference.iloc[1:], lags=18, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on seasonal first difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "#plot the ACF\n",
    "fig = sm.graphics.tsa.plot_acf(monthly_sales.seasonal_first_difference.iloc[13:], lags=24, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "#plot the PACF\n",
    "fig = sm.graphics.tsa.plot_pacf(monthly_sales.seasonal_first_difference.iloc[13:], lags=24, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is still some correlation between the times series data, we want to include terms that will account for this correlation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determinging AR and MA terms\n",
    "\n",
    "\n",
    "What’s the best way to correct for autocorrelation: adding AR terms or adding MA terms?  \n",
    "\n",
    "A rule-of-thumb for this situation, which will be discussed in more detail later on, is that positive autocorrelation is usually best treated by adding an AR term to the model and negative autocorrelation is usually best treated by adding an MA term. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./resources/armaguidelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules for Indentifying ARIMA Models\n",
    "\n",
    "https://people.duke.edu/~rnau/arimrule.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "If you are trying to predict sales of grad school prep courses, what are some exogenous features you would want to include in your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating and Diagnosing a Possible Model\n",
    "After you’ve made a guess (or two) at a possible model, use software such as R, Minitab, or SAS to estimate the coefficients. Most software will use maximum likelihood estimation methods to make the estimates. Once the model has been estimated, do the following.\n",
    "\n",
    "- Look at the significance of the coefficients. In R, sarima provides p-values and so you may simply compare the p-value to the standard 0.05 cut-off. The arima command does not provide p-values and so you can calculate a t-statistic:  = estimated coeff. / std. error of coeff. Recall that  is the Student -value with area  to the right of  on df degrees of freedom. If , then the estimated coefficient is significantly different from 0. When n is large, you may compare estimated coeff. / std. error of coeff to 1.96.\n",
    "- Look at the ACF of the residuals. For a good model, all autocorrelations for the residual series should be non-significant. If this isn’t the case, you need to try a different model.\n",
    "- Look at Box-Pierce (Ljung) tests for possible residual autocorrelation at various lags (see Lesson 3.2 for a description of this test).\n",
    "- If non-constant variance is a concern, look at a plot of residuals versus fits and/or a time series plot of the residuals.\n",
    "If something looks wrong, you’ll have to revise your guess at what the model might be. This might involve adding parameters or re-interpreting the original ACF and PACF to possibly move in a different direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if More Than One Model Looks Okay?\n",
    "Sometimes more than one model can seem to work for the same dataset. When that’s the case, some things you can do to decide between the models are:\n",
    "\n",
    "- Possibly choose the model with the fewest parameters.\n",
    "- Examine standard errors of forecast values. Pick the model with the generally lowest standard errors for predictions of the future.\n",
    "- Compare models with regard to statistics such as the MSE (the estimate of the variance of the wt), AIC, AICc, and SIC (also called BIC). Lower values of these statistics are desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.tsa.statespace.SARIMAX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                order=(0, 1, 1),\n",
    "                                seasonal_order=(1, 1, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = mod.fit()\n",
    "print(results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(18, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create predictions of  data \n",
    "#dynamic = False means that we are only predcicting one step ahead\n",
    "pred = results.get_prediction(start=pd.to_datetime('2015-06-30'), dynamic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this helps us to see the confidence intervals of our predictions\n",
    "pred_ci = pred.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot our predictions against actual data\n",
    "ax = monthly_sales['2015':].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 4))\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Retail_sold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Time Series Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use test train split when evaluating this model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://people.duke.edu/~rnau/randwkf2.gif' width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  The data in the **estimation period** are used to help select the model and to estimate its parameters. Forecasts made in this period are not completely \"honest\" because data on both sides of each observation are used to help determine the forecast. \n",
    "\n",
    "- The data in the **validation period** are held out during parameter estimation, and if you are careful you will also withhold these values during the exploratory phase of analysis when you select your model. One-step-ahead forecasts made in this period are often called backtests. Ideally, these are \"honest\" forecasts, however, if you test a great number of models and choose the model whose errors are smallest in the validation period, you may end up overfitting the data within the validation period as well as in the estimation period.\n",
    "\n",
    "- **Forecasts into the future** are \"true\" forecasts that are made for time periods beyond the end of the available data. For a model which is purely extrapolative in nature , it is possible to extend the forecasts an arbitrary number of periods into the future by \"bootstrapping\" the model: first a one-period-ahead forecast is made, then the one-period-ahead forecast is treated as a data point and the model is cranked ahead to produce a two-period-ahead forecast, and so on as far as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_forecasted = pred.predicted_mean\n",
    "y_truth = y['2015-06-30':]\n",
    "\n",
    "#calculate the MSE for the data\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('The Mean Squared Error is {}'.format(round(mse, 2)))\n",
    "print('The Root Mean Squared Error is {}'.format(round(np.sqrt(mse), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's predict all of 2016\n",
    "\n",
    "In order for this to work, we need to extend our current data frame to account for the future time periods we will predict on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_uc = results.get_forecast(steps=12)\n",
    "pred_ci = pred_uc.conf_int()\n",
    "ax = y.plot(label='observed', figsize=(14, 4))\n",
    "pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "start = datetime.datetime.strptime(\"2016-01-31\", \"%Y-%m-%d\")\n",
    "date_list = [start + relativedelta(months=x) for x in range(0,12)]\n",
    "future = pd.DataFrame(index=date_list, columns= monthly_sales.columns)\n",
    "monthly_sales = pd.concat([monthly_sales, future])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales['forecast'] = res.predict(start = 72, end = 84, dynamic= True)  \n",
    "monthly_sales[['payment_amount', 'forecast']].iloc[-24:].plot(figsize=(12, 8)) \n",
    "plt.savefig('ts_predict_future.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application practice with dataset of trasit riders in Portland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
