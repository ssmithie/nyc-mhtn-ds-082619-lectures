{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is Topic Modeling and NLP useful?\n",
    "\n",
    "- Imagine you have take all the songs that made the Top 100 billboard Charts since 1970. How could you then use topic modeling and NLP to create some  useful analysis?\n",
    "\n",
    "\n",
    "- You work for a online news aggregator that takes news articles from all over the internet and sends out important articles to the users. The problem is each website has a different way of classifying their news stories. How could topic modeling and NLP to help you?\n",
    "\n",
    "\n",
    "- You work for Ted Talks and want to predict the number of views each video will get.  Right now you only have the text of the speeches, their length, and the number of views. How could you use NLP and topic modelling to help with your predictive model?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/swilson5/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'rec.motorcycles' 'misc.forsale'\n",
      " 'comp.os.ms-windows.misc' 'alt.atheism' 'comp.graphics'\n",
      " 'rec.sport.baseball' 'rec.sport.hockey' 'sci.electronics' 'sci.space'\n",
      " 'talk.politics.misc' 'sci.med' 'talk.politics.mideast'\n",
      " 'soc.religion.christian' 'comp.windows.x' 'comp.sys.ibm.pc.hardware'\n",
      " 'talk.politics.guns' 'talk.religion.misc' 'sci.crypt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>From: a207706@moe.dseg.ti.com (Robert Loper)\\n...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>From: kimman@magnus.acs.ohio-state.edu (Kim Ri...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>Subject: Re: Don't more innocents die without ...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>From: livesey@solntze.wpd.sgi.com (Jon Livesey...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>From: dls@aeg.dsto.gov.au (David Silver)\\nSubj...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>Subject: Re: Mike Francesa's 1993 Predictions\\...</td>\n",
       "      <td>9</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>From: jet@netcom.Netcom.COM (J. Eric Townsend)...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>From: gld@cunixb.cc.columbia.edu (Gary L Dare)...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>From: sehari@iastate.edu (Babak Sehari)\\nSubje...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>From: danmg@grok85.ColumbiaSC.NCR.COM (Daniel ...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>From: henry@zoo.toronto.edu (Henry Spencer)\\nS...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>From: tzs@stein2.u.washington.edu (Tim Smith)\\...</td>\n",
       "      <td>18</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>From: U56149@uicvm.uic.edu\\nSubject: LCIII &amp; M...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>From: nsmca@aurora.alaska.edu\\nSubject: Lunar ...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>From: acooper@mac.cc.macalstr.edu\\nSubject: Re...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10015</th>\n",
       "      <td>From: billc@col.hp.com (Bill Claussen)\\nSubjec...</td>\n",
       "      <td>13</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>From: vestman@cs.umu.se (Peter Vestman)\\nSubje...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10017</th>\n",
       "      <td>From: nstramer@supergas.dazixco.ingr.com (Naft...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>From: kohli@ecs.umass.edu\\nSubject: Mazda GLC ...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>From: mussack@austin.ibm.com (Christopher Muss...</td>\n",
       "      <td>15</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...</td>\n",
       "      <td>18</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>From: noye@midway.uchicago.edu (vera shanti no...</td>\n",
       "      <td>15</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10021</th>\n",
       "      <td>From:  (Sean Garrison)\\nSubject: Re: WFAN\\nNnt...</td>\n",
       "      <td>9</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10022</th>\n",
       "      <td>From: christy@cs.concordia.ca (Christy)\\nSubje...</td>\n",
       "      <td>5</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  target  \\\n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "10     From: irwin@cmptrc.lonestar.org (Irwin Arnstei...       8   \n",
       "100    From: tchen@magnus.acs.ohio-state.edu (Tsung-K...       6   \n",
       "1000   From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...       2   \n",
       "10000  From: a207706@moe.dseg.ti.com (Robert Loper)\\n...       7   \n",
       "10001  From: kimman@magnus.acs.ohio-state.edu (Kim Ri...       6   \n",
       "10002  From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...       2   \n",
       "10003  Subject: Re: Don't more innocents die without ...       0   \n",
       "10004  From: livesey@solntze.wpd.sgi.com (Jon Livesey...       0   \n",
       "10005  From: dls@aeg.dsto.gov.au (David Silver)\\nSubj...       1   \n",
       "10006  Subject: Re: Mike Francesa's 1993 Predictions\\...       9   \n",
       "10007  From: jet@netcom.Netcom.COM (J. Eric Townsend)...       8   \n",
       "10008  From: gld@cunixb.cc.columbia.edu (Gary L Dare)...      10   \n",
       "10009  From: sehari@iastate.edu (Babak Sehari)\\nSubje...      12   \n",
       "1001   From: danmg@grok85.ColumbiaSC.NCR.COM (Daniel ...       7   \n",
       "10010  From: henry@zoo.toronto.edu (Henry Spencer)\\nS...      14   \n",
       "10011  From: tzs@stein2.u.washington.edu (Tim Smith)\\...      18   \n",
       "10012  From: U56149@uicvm.uic.edu\\nSubject: LCIII & M...       4   \n",
       "10013  From: nsmca@aurora.alaska.edu\\nSubject: Lunar ...      14   \n",
       "10014  From: acooper@mac.cc.macalstr.edu\\nSubject: Re...       0   \n",
       "10015  From: billc@col.hp.com (Bill Claussen)\\nSubjec...      13   \n",
       "10016  From: vestman@cs.umu.se (Peter Vestman)\\nSubje...       2   \n",
       "10017  From: nstramer@supergas.dazixco.ingr.com (Naft...      17   \n",
       "10018  From: kohli@ecs.umass.edu\\nSubject: Mazda GLC ...       6   \n",
       "10019  From: mussack@austin.ibm.com (Christopher Muss...      15   \n",
       "1002   From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...      18   \n",
       "10020  From: noye@midway.uchicago.edu (vera shanti no...      15   \n",
       "10021  From:  (Sean Garrison)\\nSubject: Re: WFAN\\nNnt...       9   \n",
       "10022  From: christy@cs.concordia.ca (Christy)\\nSubje...       5   \n",
       "\n",
       "                  target_names  \n",
       "0                    rec.autos  \n",
       "1        comp.sys.mac.hardware  \n",
       "10             rec.motorcycles  \n",
       "100               misc.forsale  \n",
       "1000   comp.os.ms-windows.misc  \n",
       "10000                rec.autos  \n",
       "10001             misc.forsale  \n",
       "10002  comp.os.ms-windows.misc  \n",
       "10003              alt.atheism  \n",
       "10004              alt.atheism  \n",
       "10005            comp.graphics  \n",
       "10006       rec.sport.baseball  \n",
       "10007          rec.motorcycles  \n",
       "10008         rec.sport.hockey  \n",
       "10009          sci.electronics  \n",
       "1001                 rec.autos  \n",
       "10010                sci.space  \n",
       "10011       talk.politics.misc  \n",
       "10012    comp.sys.mac.hardware  \n",
       "10013                sci.space  \n",
       "10014              alt.atheism  \n",
       "10015                  sci.med  \n",
       "10016  comp.os.ms-windows.misc  \n",
       "10017    talk.politics.mideast  \n",
       "10018             misc.forsale  \n",
       "10019   soc.religion.christian  \n",
       "1002        talk.politics.misc  \n",
       "10020   soc.religion.christian  \n",
       "10021       rec.sport.baseball  \n",
       "10022           comp.windows.x  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explanation Resources \n",
    "\n",
    "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\n",
    "\n",
    "https://www.youtube.com/watch?v=DWJYZq_fQ2A\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.machinelearningplus.com/wp-content/uploads/2018/03/Inferring-Topic-from-Keywords-1024x666.png' img/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d8b43408e7b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(text, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    word = WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "#     print('token',word)\n",
    "    return stemmer.stem(word)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a document to preview after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['From:', 'lerxst@wam.umd.edu', \"(where's\", 'my', 'thing)\\nSubject:', 'WHAT', 'car', 'is', 'this!?\\nNntp-Posting-Host:', 'rac3.wam.umd.edu\\nOrganization:', 'University', 'of', 'Maryland,', 'College', 'Park\\nLines:', '15\\n\\n', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw\\nthe', 'other', 'day.', 'It', 'was', 'a', '2-door', 'sports', 'car,', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/\\nearly', '70s.', 'It', 'was', 'called', 'a', 'Bricklin.', 'The', 'doors', 'were', 'really', 'small.', 'In', 'addition,\\nthe', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body.', 'This', 'is', '\\nall', 'I', 'know.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name,', 'engine', 'specs,', 'years\\nof', 'production,', 'where', 'this', 'car', 'is', 'made,', 'history,', 'or', 'whatever', 'info', 'you\\nhave', 'on', 'this', 'funky', 'looking', 'car,', 'please', 'e-mail.\\n\\nThanks,\\n-', 'IL\\n', '', '', '----', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '----\\n\\n\\n\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = df.iloc[0].values[0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = df['content'].map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [lerxst, thing, subject, nntp, post, host, org...\n",
       "1        [guykuo, carson, washington, subject, clock, p...\n",
       "10       [irwin, cmptrc, lonestar, irwin, arnstein, sub...\n",
       "100      [tchen, magnus, ohio, state, tsung, chen, subj...\n",
       "1000     [dabl, lindbergh, subject, diamond, mous, curs...\n",
       "10000    [dseg, robert, loper, subject, nntp, post, hos...\n",
       "10001    [kimman, magnus, ohio, state, richard, subject...\n",
       "10002    [kwilson, casbah, acn, kirtley, wilson, subjec...\n",
       "10003    [subject, innoc, death, penalti, bobb, vice, r...\n",
       "10004    [livesey, solntz, livesey, subject, genocid, c...\n",
       "10005    [dsto, david, silver, subject, fractal, genera...\n",
       "10006    [subject, mike, francesa, predict, gajarski, p...\n",
       "10007    [netcom, netcom, eric, townsend, subject, insu...\n",
       "10008    [cunixb, columbia, gari, dare, subject, covera...\n",
       "10009    [sehari, iastat, babak, sehari, subject, disk,...\n",
       "1001     [danmg, grok, columbiasc, daniel, adam, subjec...\n",
       "10010    [henri, toronto, henri, spencer, subject, luna...\n",
       "10011    [stein, washington, smith, subject, tie, abort...\n",
       "10012    [uicvm, subject, lciii, midi, articl, uicvm, o...\n",
       "10013    [nsmca, aurora, alaska, subject, lunar, coloni...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words on the Data set\n",
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addit\n",
      "1 bodi\n",
      "2 bricklin\n",
      "3 bring\n",
      "4 bumper\n",
      "5 call\n",
      "6 colleg\n",
      "7 door\n",
      "8 earli\n",
      "9 engin\n",
      "10 enlighten\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out tokens that appear in:\n",
    "\n",
    "- less than 15 documents (absolute number) \n",
    "- more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "- after the above two steps, keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=20, no_above=0.66, keep_n=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6591"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow\n",
    "\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 1),\n",
       " (20, 1),\n",
       " (26, 1),\n",
       " (30, 1),\n",
       " (138, 1),\n",
       " (142, 3),\n",
       " (244, 1),\n",
       " (271, 5),\n",
       " (403, 2),\n",
       " (593, 1),\n",
       " (598, 1),\n",
       " (630, 1),\n",
       " (657, 1),\n",
       " (701, 1),\n",
       " (815, 1),\n",
       " (836, 1),\n",
       " (877, 1),\n",
       " (921, 1),\n",
       " (985, 1),\n",
       " (1046, 1),\n",
       " (1388, 1),\n",
       " (1397, 1),\n",
       " (1568, 1),\n",
       " (1617, 1),\n",
       " (1685, 1),\n",
       " (1839, 2),\n",
       " (1897, 1),\n",
       " (2004, 1),\n",
       " (2502, 1),\n",
       " (2662, 1),\n",
       " (2709, 1),\n",
       " (2847, 1),\n",
       " (2957, 1),\n",
       " (3303, 1),\n",
       " (4583, 1),\n",
       " (4806, 1),\n",
       " (5079, 2),\n",
       " (5763, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview Bag Of Words for our sample preprocessed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 11 (\"host\") appears 1 time.\n",
      "Word 20 (\"nntp\") appears 1 time.\n",
      "Word 26 (\"spec\") appears 1 time.\n",
      "Word 30 (\"univers\") appears 1 time.\n",
      "Word 138 (\"state\") appears 1 time.\n",
      "Word 142 (\"window\") appears 3 time.\n",
      "Word 244 (\"richard\") appears 1 time.\n",
      "Word 271 (\"program\") appears 5 time.\n",
      "Word 403 (\"silver\") appears 2 time.\n",
      "Word 593 (\"secur\") appears 1 time.\n",
      "Word 598 (\"true\") appears 1 time.\n",
      "Word 630 (\"gate\") appears 1 time.\n",
      "Word 657 (\"econom\") appears 1 time.\n",
      "Word 701 (\"high\") appears 1 time.\n",
      "Word 815 (\"support\") appears 1 time.\n",
      "Word 836 (\"meet\") appears 1 time.\n",
      "Word 877 (\"correct\") appears 1 time.\n",
      "Word 921 (\"task\") appears 1 time.\n",
      "Word 985 (\"current\") appears 1 time.\n",
      "Word 1046 (\"major\") appears 1 time.\n",
      "Word 1388 (\"oper\") appears 1 time.\n",
      "Word 1397 (\"promis\") appears 1 time.\n",
      "Word 1568 (\"dept\") appears 1 time.\n",
      "Word 1617 (\"server\") appears 1 time.\n",
      "Word 1685 (\"user\") appears 1 time.\n",
      "Word 1839 (\"multi\") appears 2 time.\n",
      "Word 1897 (\"expect\") appears 1 time.\n",
      "Word 2004 (\"assur\") appears 1 time.\n",
      "Word 2502 (\"jose\") appears 1 time.\n",
      "Word 2662 (\"processor\") appears 1 time.\n",
      "Word 2709 (\"math\") appears 1 time.\n",
      "Word 2847 (\"rick\") appears 1 time.\n",
      "Word 2957 (\"overhead\") appears 1 time.\n",
      "Word 3303 (\"warner\") appears 1 time.\n",
      "Word 4583 (\"giant\") appears 1 time.\n",
      "Word 4806 (\"primarili\") appears 1 time.\n",
      "Word 5079 (\"billi\") appears 2 time.\n",
      "Word 5763 (\"app\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.16531831488632115),\n",
      " (1, 0.1678553823993299),\n",
      " (2, 0.15020052155842978),\n",
      " (3, 0.28581344134222897),\n",
      " (4, 0.11439130765694391),\n",
      " (5, 0.1516798683845623),\n",
      " (6, 0.3893283888350302),\n",
      " (7, 0.16890780449478127),\n",
      " (8, 0.12279484913135752),\n",
      " (9, 0.2634573656114004),\n",
      " (10, 0.16446021701525967),\n",
      " (11, 0.041907566184917734),\n",
      " (12, 0.13943742962367772),\n",
      " (13, 0.053228561863657146),\n",
      " (14, 0.17840678372959912),\n",
      " (15, 0.1614581045972935),\n",
      " (16, 0.10182359643822467),\n",
      " (17, 0.23339500537007382),\n",
      " (18, 0.1622034571062096),\n",
      " (19, 0.28046098400184816),\n",
      " (20, 0.04264750560541665),\n",
      " (21, 0.18494250912032378),\n",
      " (22, 0.14867573439400095),\n",
      " (23, 0.15971285457776704),\n",
      " (24, 0.18156677399111007),\n",
      " (25, 0.14477104789605966),\n",
      " (26, 0.20972103713602588),\n",
      " (27, 0.19290536120043997),\n",
      " (28, 0.08432238906696132),\n",
      " (29, 0.08377240739564006),\n",
      " (30, 0.04441833427243015),\n",
      " (31, 0.13721646110054028),\n",
      " (32, 0.08417832496773713)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words\n",
    "\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of processes must be at least 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-65714e9076a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMulticore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training LDA model using %i processes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_e_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_queue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpass_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mqueue_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreallen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36mPool\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         return Pool(processes, initializer, initargs, maxtasksperchild,\n\u001b[0;32m--> 119\u001b[0;31m                     context=self.get_context())\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mRawValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypecode_or_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of processes must be at least 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitializer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of processes must be at least 1"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=4, workers=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.008*\"think\" + 0.007*\"know\" + 0.007*\"peopl\" + 0.006*\"wire\" + 0.006*\"articl\" + 0.006*\"like\" + 0.006*\"point\" + 0.005*\"problem\" + 0.004*\"question\" + 0.004*\"time\"\n",
      "Topic: 1 \n",
      "Words: 0.007*\"like\" + 0.006*\"columbia\" + 0.006*\"articl\" + 0.006*\"nntp\" + 0.006*\"host\" + 0.005*\"univers\" + 0.005*\"think\" + 0.004*\"team\" + 0.004*\"know\" + 0.004*\"peopl\"\n",
      "Topic: 2 \n",
      "Words: 0.006*\"server\" + 0.006*\"like\" + 0.005*\"think\" + 0.005*\"unit\" + 0.005*\"window\" + 0.004*\"state\" + 0.004*\"appear\" + 0.004*\"articl\" + 0.004*\"time\" + 0.004*\"file\"\n",
      "Topic: 3 \n",
      "Words: 0.007*\"articl\" + 0.007*\"say\" + 0.006*\"like\" + 0.006*\"good\" + 0.006*\"peopl\" + 0.005*\"know\" + 0.005*\"time\" + 0.005*\"think\" + 0.005*\"come\" + 0.004*\"need\"\n",
      "Topic: 4 \n",
      "Words: 0.008*\"articl\" + 0.008*\"like\" + 0.007*\"peopl\" + 0.007*\"univers\" + 0.007*\"think\" + 0.006*\"know\" + 0.005*\"year\" + 0.005*\"right\" + 0.004*\"time\" + 0.004*\"host\"\n",
      "Topic: 5 \n",
      "Words: 0.006*\"peopl\" + 0.006*\"like\" + 0.006*\"articl\" + 0.005*\"think\" + 0.005*\"year\" + 0.004*\"mail\" + 0.004*\"good\" + 0.004*\"host\" + 0.004*\"nntp\" + 0.004*\"internet\"\n",
      "Topic: 6 \n",
      "Words: 0.009*\"know\" + 0.008*\"pitt\" + 0.007*\"articl\" + 0.007*\"gordon\" + 0.006*\"like\" + 0.006*\"bank\" + 0.006*\"good\" + 0.005*\"time\" + 0.005*\"year\" + 0.005*\"think\"\n",
      "Topic: 7 \n",
      "Words: 0.017*\"drive\" + 0.009*\"disk\" + 0.008*\"articl\" + 0.008*\"state\" + 0.007*\"univers\" + 0.007*\"host\" + 0.007*\"nntp\" + 0.006*\"ohio\" + 0.006*\"control\" + 0.005*\"hard\"\n",
      "Topic: 8 \n",
      "Words: 0.008*\"host\" + 0.008*\"nntp\" + 0.008*\"like\" + 0.007*\"univers\" + 0.006*\"scsi\" + 0.006*\"know\" + 0.006*\"color\" + 0.005*\"good\" + 0.005*\"drive\" + 0.005*\"window\"\n",
      "Topic: 9 \n",
      "Words: 0.018*\"file\" + 0.015*\"window\" + 0.006*\"think\" + 0.005*\"know\" + 0.005*\"time\" + 0.005*\"host\" + 0.005*\"articl\" + 0.005*\"peopl\" + 0.005*\"list\" + 0.004*\"mail\"\n",
      "Topic: 10 \n",
      "Words: 0.009*\"articl\" + 0.006*\"host\" + 0.006*\"nntp\" + 0.006*\"like\" + 0.006*\"peopl\" + 0.005*\"know\" + 0.004*\"work\" + 0.004*\"think\" + 0.004*\"right\" + 0.004*\"univers\"\n",
      "Topic: 11 \n",
      "Words: 0.011*\"articl\" + 0.010*\"univers\" + 0.008*\"know\" + 0.007*\"host\" + 0.007*\"nntp\" + 0.006*\"world\" + 0.005*\"distribut\" + 0.005*\"window\" + 0.005*\"mail\" + 0.005*\"repli\"\n",
      "Topic: 12 \n",
      "Words: 0.010*\"peopl\" + 0.008*\"think\" + 0.007*\"articl\" + 0.006*\"come\" + 0.006*\"time\" + 0.005*\"univers\" + 0.005*\"say\" + 0.005*\"like\" + 0.005*\"moral\" + 0.004*\"believ\"\n",
      "Topic: 13 \n",
      "Words: 0.010*\"peopl\" + 0.009*\"know\" + 0.008*\"say\" + 0.007*\"articl\" + 0.007*\"go\" + 0.007*\"think\" + 0.006*\"like\" + 0.006*\"time\" + 0.006*\"right\" + 0.005*\"state\"\n",
      "Topic: 14 \n",
      "Words: 0.010*\"program\" + 0.007*\"work\" + 0.007*\"window\" + 0.006*\"like\" + 0.006*\"card\" + 0.006*\"need\" + 0.006*\"imag\" + 0.006*\"univers\" + 0.006*\"time\" + 0.006*\"articl\"\n",
      "Topic: 15 \n",
      "Words: 0.017*\"game\" + 0.012*\"team\" + 0.009*\"play\" + 0.008*\"year\" + 0.008*\"univers\" + 0.007*\"hockey\" + 0.006*\"host\" + 0.006*\"season\" + 0.006*\"nntp\" + 0.005*\"player\"\n",
      "Topic: 16 \n",
      "Words: 0.018*\"space\" + 0.013*\"nasa\" + 0.008*\"articl\" + 0.006*\"host\" + 0.006*\"nntp\" + 0.006*\"univers\" + 0.005*\"file\" + 0.005*\"drive\" + 0.005*\"time\" + 0.005*\"work\"\n",
      "Topic: 17 \n",
      "Words: 0.011*\"jesus\" + 0.011*\"christian\" + 0.008*\"think\" + 0.008*\"say\" + 0.007*\"know\" + 0.007*\"believ\" + 0.006*\"univers\" + 0.005*\"peopl\" + 0.005*\"exist\" + 0.004*\"thing\"\n",
      "Topic: 18 \n",
      "Words: 0.022*\"armenian\" + 0.012*\"turkish\" + 0.008*\"peopl\" + 0.007*\"articl\" + 0.007*\"turk\" + 0.006*\"right\" + 0.006*\"armenia\" + 0.005*\"world\" + 0.005*\"argic\" + 0.005*\"like\"\n",
      "Topic: 19 \n",
      "Words: 0.010*\"encrypt\" + 0.009*\"chip\" + 0.007*\"clipper\" + 0.006*\"articl\" + 0.006*\"secur\" + 0.006*\"govern\" + 0.005*\"know\" + 0.005*\"peopl\" + 0.004*\"year\" + 0.004*\"think\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.004*\"window\" + 0.002*\"file\" + 0.002*\"dyer\" + 0.002*\"alaska\" + 0.002*\"sale\" + 0.002*\"program\" + 0.002*\"card\" + 0.002*\"know\" + 0.002*\"problem\" + 0.002*\"like\"\n",
      "Topic: 1 Word: 0.003*\"buffalo\" + 0.002*\"virginia\" + 0.002*\"jesus\" + 0.002*\"peopl\" + 0.002*\"bontchev\" + 0.002*\"univers\" + 0.002*\"hamburg\" + 0.002*\"cramer\" + 0.002*\"articl\" + 0.002*\"brian\"\n",
      "Topic: 2 Word: 0.002*\"psuvm\" + 0.002*\"drive\" + 0.002*\"window\" + 0.002*\"univers\" + 0.002*\"know\" + 0.002*\"card\" + 0.002*\"caltech\" + 0.002*\"think\" + 0.002*\"game\" + 0.002*\"iastat\"\n",
      "Topic: 3 Word: 0.003*\"card\" + 0.003*\"scsi\" + 0.003*\"wire\" + 0.002*\"driver\" + 0.002*\"access\" + 0.002*\"diamond\" + 0.002*\"univers\" + 0.002*\"hook\" + 0.002*\"christian\" + 0.002*\"video\"\n",
      "Topic: 4 Word: 0.004*\"sandvik\" + 0.003*\"drive\" + 0.002*\"card\" + 0.002*\"netcom\" + 0.002*\"david\" + 0.002*\"appl\" + 0.002*\"clipper\" + 0.002*\"kent\" + 0.002*\"softwar\" + 0.002*\"inform\"\n",
      "Topic: 5 Word: 0.003*\"hulman\" + 0.002*\"window\" + 0.002*\"file\" + 0.002*\"simm\" + 0.002*\"univers\" + 0.002*\"widget\" + 0.002*\"rise\" + 0.002*\"program\" + 0.002*\"host\" + 0.002*\"nntp\"\n",
      "Topic: 6 Word: 0.005*\"gordon\" + 0.004*\"pitt\" + 0.004*\"bank\" + 0.002*\"articl\" + 0.002*\"cadr\" + 0.002*\"think\" + 0.002*\"right\" + 0.002*\"surrend\" + 0.002*\"peopl\" + 0.002*\"chastiti\"\n",
      "Topic: 7 Word: 0.003*\"game\" + 0.002*\"keith\" + 0.002*\"muenchen\" + 0.002*\"drive\" + 0.002*\"okcforum\" + 0.002*\"columbia\" + 0.002*\"like\" + 0.002*\"univers\" + 0.002*\"know\" + 0.002*\"caltech\"\n",
      "Topic: 8 Word: 0.003*\"window\" + 0.003*\"chip\" + 0.003*\"intercon\" + 0.003*\"duke\" + 0.002*\"amanda\" + 0.002*\"uiuc\" + 0.002*\"encrypt\" + 0.002*\"austin\" + 0.002*\"clipper\" + 0.002*\"shearson\"\n",
      "Topic: 9 Word: 0.004*\"disk\" + 0.003*\"armenian\" + 0.003*\"drive\" + 0.003*\"serdar\" + 0.003*\"argic\" + 0.002*\"floppi\" + 0.002*\"file\" + 0.002*\"univers\" + 0.002*\"window\" + 0.002*\"know\"\n",
      "Topic: 10 Word: 0.004*\"window\" + 0.003*\"bike\" + 0.003*\"columbia\" + 0.003*\"chip\" + 0.002*\"sdsu\" + 0.002*\"larc\" + 0.002*\"cunixb\" + 0.002*\"clipper\" + 0.002*\"escrow\" + 0.002*\"univers\"\n",
      "Topic: 11 Word: 0.003*\"window\" + 0.003*\"seizur\" + 0.003*\"tammi\" + 0.002*\"walla\" + 0.002*\"healta\" + 0.002*\"david\" + 0.002*\"duke\" + 0.002*\"cray\" + 0.002*\"univers\" + 0.002*\"look\"\n",
      "Topic: 12 Word: 0.003*\"peopl\" + 0.002*\"christian\" + 0.002*\"window\" + 0.002*\"know\" + 0.002*\"think\" + 0.002*\"homosexu\" + 0.002*\"world\" + 0.002*\"like\" + 0.002*\"polygon\" + 0.002*\"univers\"\n",
      "Topic: 13 Word: 0.004*\"israel\" + 0.003*\"isra\" + 0.003*\"arab\" + 0.002*\"peopl\" + 0.002*\"game\" + 0.002*\"right\" + 0.002*\"team\" + 0.002*\"think\" + 0.002*\"state\" + 0.002*\"year\"\n",
      "Topic: 14 Word: 0.004*\"window\" + 0.003*\"nasa\" + 0.003*\"toronto\" + 0.002*\"henri\" + 0.002*\"file\" + 0.002*\"program\" + 0.002*\"know\" + 0.002*\"thank\" + 0.002*\"color\" + 0.002*\"univers\"\n",
      "Topic: 15 Word: 0.004*\"jesus\" + 0.004*\"stratus\" + 0.003*\"christian\" + 0.002*\"believ\" + 0.002*\"peopl\" + 0.002*\"say\" + 0.002*\"church\" + 0.002*\"think\" + 0.002*\"bibl\" + 0.002*\"uchicago\"\n",
      "Topic: 16 Word: 0.004*\"upenn\" + 0.003*\"mail\" + 0.003*\"printer\" + 0.003*\"henri\" + 0.003*\"tempest\" + 0.002*\"msstate\" + 0.002*\"rochest\" + 0.002*\"univers\" + 0.002*\"window\" + 0.002*\"thank\"\n",
      "Topic: 17 Word: 0.004*\"encrypt\" + 0.003*\"govern\" + 0.003*\"chip\" + 0.002*\"captain\" + 0.002*\"like\" + 0.002*\"clipper\" + 0.002*\"stanford\" + 0.002*\"peopl\" + 0.002*\"univers\" + 0.002*\"know\"\n",
      "Topic: 18 Word: 0.002*\"year\" + 0.002*\"know\" + 0.002*\"team\" + 0.002*\"drive\" + 0.002*\"window\" + 0.002*\"game\" + 0.002*\"time\" + 0.002*\"space\" + 0.002*\"like\" + 0.002*\"mous\"\n",
      "Topic: 19 Word: 0.002*\"host\" + 0.002*\"uiuc\" + 0.002*\"nntp\" + 0.002*\"file\" + 0.002*\"know\" + 0.002*\"univers\" + 0.002*\"articl\" + 0.002*\"say\" + 0.002*\"christian\" + 0.002*\"cleveland\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "We will check where our test document would be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graham',\n",
       " 'toal',\n",
       " 'gtoal',\n",
       " 'gtoal',\n",
       " 'subject',\n",
       " 'hard',\n",
       " 'drive',\n",
       " 'secur',\n",
       " 'target',\n",
       " 'origin',\n",
       " 'gtoal',\n",
       " 'pizzabox',\n",
       " 'demon',\n",
       " 'keyword',\n",
       " 'entropi',\n",
       " 'nntp',\n",
       " 'post',\n",
       " 'host',\n",
       " 'pizzabox',\n",
       " 'demon',\n",
       " 'repli',\n",
       " 'graham',\n",
       " 'toal',\n",
       " 'gtoal',\n",
       " 'gtoal',\n",
       " 'organ',\n",
       " 'cuddlehog',\n",
       " 'anonym',\n",
       " 'line',\n",
       " 'articl',\n",
       " 'kean',\n",
       " 'write',\n",
       " 'matter',\n",
       " 'fact',\n",
       " 'random',\n",
       " 'file',\n",
       " 'disk',\n",
       " 'reason',\n",
       " 'special',\n",
       " 'purpos',\n",
       " 'hardwar',\n",
       " 'take',\n",
       " 'long',\n",
       " 'time',\n",
       " 'generat',\n",
       " 'good',\n",
       " 'random',\n",
       " 'bit',\n",
       " 'program',\n",
       " 'crank',\n",
       " 'coupl',\n",
       " 'bit',\n",
       " 'minut',\n",
       " 'pretti',\n",
       " 'conserv',\n",
       " 'time',\n",
       " 'need',\n",
       " 'sound',\n",
       " 'like',\n",
       " 'use',\n",
       " 'program',\n",
       " 'interest',\n",
       " 'post',\n",
       " 'sourc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7221401333808899\t \n",
      "Topic: 0.010*\"program\" + 0.007*\"work\" + 0.007*\"window\" + 0.006*\"like\" + 0.006*\"card\" + 0.006*\"need\" + 0.006*\"imag\" + 0.006*\"univers\" + 0.006*\"time\" + 0.006*\"articl\"\n",
      "\n",
      "Score: 0.17414003610610962\t \n",
      "Topic: 0.018*\"file\" + 0.015*\"window\" + 0.006*\"think\" + 0.005*\"know\" + 0.005*\"time\" + 0.005*\"host\" + 0.005*\"articl\" + 0.005*\"peopl\" + 0.005*\"list\" + 0.004*\"mail\"\n",
      "\n",
      "Score: 0.0860012099146843\t \n",
      "Topic: 0.017*\"game\" + 0.012*\"team\" + 0.009*\"play\" + 0.008*\"year\" + 0.008*\"univers\" + 0.007*\"hockey\" + 0.006*\"host\" + 0.006*\"season\" + 0.006*\"nntp\" + 0.005*\"player\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7045379877090454\t \n",
      "Topic: 0.003*\"peopl\" + 0.002*\"christian\" + 0.002*\"window\" + 0.002*\"know\" + 0.002*\"think\" + 0.002*\"homosexu\" + 0.002*\"world\" + 0.002*\"like\" + 0.002*\"polygon\" + 0.002*\"univers\"\n",
      "\n",
      "Score: 0.2691507339477539\t \n",
      "Topic: 0.004*\"window\" + 0.002*\"file\" + 0.002*\"dyer\" + 0.002*\"alaska\" + 0.002*\"sale\" + 0.002*\"program\" + 0.002*\"card\" + 0.002*\"know\" + 0.002*\"problem\" + 0.002*\"like\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.008*\"think\" + 0.007*\"know\" + 0.007*\"peopl\" + 0.006*\"wire\" + 0.006*\"articl\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.007*\"like\" + 0.006*\"columbia\" + 0.006*\"articl\" + 0.006*\"nntp\" + 0.006*\"host\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.006*\"server\" + 0.006*\"like\" + 0.005*\"think\" + 0.005*\"unit\" + 0.005*\"window\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.007*\"articl\" + 0.007*\"say\" + 0.006*\"like\" + 0.006*\"good\" + 0.006*\"peopl\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.008*\"articl\" + 0.008*\"like\" + 0.007*\"peopl\" + 0.007*\"univers\" + 0.007*\"think\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.006*\"peopl\" + 0.006*\"like\" + 0.006*\"articl\" + 0.005*\"think\" + 0.005*\"year\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.009*\"know\" + 0.008*\"pitt\" + 0.007*\"articl\" + 0.007*\"gordon\" + 0.006*\"like\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.017*\"drive\" + 0.009*\"disk\" + 0.008*\"articl\" + 0.008*\"state\" + 0.007*\"univers\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.008*\"host\" + 0.008*\"nntp\" + 0.008*\"like\" + 0.007*\"univers\" + 0.006*\"scsi\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.018*\"file\" + 0.015*\"window\" + 0.006*\"think\" + 0.005*\"know\" + 0.005*\"time\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.009*\"articl\" + 0.006*\"host\" + 0.006*\"nntp\" + 0.006*\"like\" + 0.006*\"peopl\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.011*\"articl\" + 0.010*\"univers\" + 0.008*\"know\" + 0.007*\"host\" + 0.007*\"nntp\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.010*\"peopl\" + 0.008*\"think\" + 0.007*\"articl\" + 0.006*\"come\" + 0.006*\"time\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.010*\"peopl\" + 0.009*\"know\" + 0.008*\"say\" + 0.007*\"articl\" + 0.007*\"go\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.010*\"program\" + 0.007*\"work\" + 0.007*\"window\" + 0.006*\"like\" + 0.006*\"card\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.017*\"game\" + 0.012*\"team\" + 0.009*\"play\" + 0.008*\"year\" + 0.008*\"univers\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.018*\"space\" + 0.013*\"nasa\" + 0.008*\"articl\" + 0.006*\"host\" + 0.006*\"nntp\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.011*\"jesus\" + 0.011*\"christian\" + 0.008*\"think\" + 0.008*\"say\" + 0.007*\"know\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.022*\"armenian\" + 0.012*\"turkish\" + 0.008*\"peopl\" + 0.007*\"articl\" + 0.007*\"turk\"\n",
      "\n",
      " Score: 0.05000000074505806\t Topic: 0.010*\"encrypt\" + 0.009*\"chip\" + 0.007*\"clipper\" + 0.006*\"articl\" + 0.006*\"secur\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = ''\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\n Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Model Perplexity and Coherence Score\n",
    "Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is.  Topic coherence score, in particular, has been more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.769944469800711\n",
      "\n",
      "Coherence Score:  0.43388944506097127\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(bow_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "    https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing this with sklearn\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "\n",
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
